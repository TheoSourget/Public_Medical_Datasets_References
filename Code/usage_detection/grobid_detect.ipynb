{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import bs4 as bs\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grobid extraction information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pdf with complete extraction: 2678\n",
      "Number of pdf with error extraction: 0\n",
      "Total number of paper: 2678\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of pdf with complete extraction:\",len(glob.glob(\"../../Results/extraction/grobid_extraction/*.xml\")))\n",
    "print(\"Number of pdf with error extraction:\",len(glob.glob(\"../../Results/extraction/grobid_extraction/*.txt\")))\n",
    "print(\"Total number of paper:\",len(glob.glob(\"../../Results/extraction/fulltext/*.pdf\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACDC': {'doi': '10.1109/TMI.2018.2837502',\n",
       "  'title': 'Deep Learning Techniques for Automatic MRI Cardiac Multi-Structures Segmentation and Diagnosis: Is the Problem Solved?',\n",
       "  'name': 'ACDC',\n",
       "  'aliases': ['ACDC', 'Automated Cardiac Diagnosis Challenge', 'AC17'],\n",
       "  'url': ['https://www.creatis.insa-lyon.fr/Challenge/acdc',\n",
       "   'https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html']},\n",
       " 'Sunnybrook': {'doi': 'https://doi.org/10.54294/g80ruo',\n",
       "  'title': 'Evaluation Framework for Algorithms Segmenting Short Axis Cardiac MRI',\n",
       "  'name': 'Sunnybrook',\n",
       "  'aliases': ['Sunnybrook'],\n",
       "  'url': ['https://www.cardiacatlas.org/sunnybrook-cardiac-data']},\n",
       " 'STACOM’11': {'doi': '10.1007/978-3-642-28326-0_9',\n",
       "  'title': 'Left Ventricular Segmentation Challenge from Cardiac MRI: A Collation Study',\n",
       "  'name': 'STACOM’11',\n",
       "  'aliases': ['STACOM’11', \"STACOM'11\"],\n",
       "  'url': ['https://www.satdl.com/download/37618']},\n",
       " 'RVSC': {'doi': 'https://doi.org/10.1016/j.media.2014.10.004',\n",
       "  'title': 'Right ventricle segmentation from cardiac MRI: A collation study',\n",
       "  'name': 'RVSC',\n",
       "  'aliases': ['RVSC'],\n",
       "  'url': ['https://rvsc.projets.litislab.fr']},\n",
       " 'M&Ms': {'doi': '10.1109/tmi.2021.3090082',\n",
       "  'title': 'Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Segmentation: The M&Ms Challenge',\n",
       "  'name': 'M&Ms',\n",
       "  'aliases': ['M&Ms'],\n",
       "  'url': ['https://www.ub.edu/mnms']},\n",
       " 'BRATS': {'doi': '10.1109/tmi.2014.2377694',\n",
       "  'title': 'The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)',\n",
       "  'name': 'BRATS',\n",
       "  'aliases': ['BRATS', 'BraTs', 'BraTS', 'Brats'],\n",
       "  'url': ['http://braintumorsegmentation.org']},\n",
       " 'PROMISE12': {'doi': '10.1016/j.media.2013.12.002',\n",
       "  'title': 'Evaluation of prostate segmentation algorithms for MRI: The PROMISE12 challenge',\n",
       "  'name': 'PROMISE12',\n",
       "  'aliases': ['PROMISE12', 'MICCAI2012 prostate MRI'],\n",
       "  'url': ['https://promise12.grand-challenge.org']},\n",
       " 'LIDC-IDRI': {'doi': '10.1118/1.3528204,10.1016/j.media.2017.06.015',\n",
       "  'title': 'The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): A Completed Reference Database of Lung Nodules on CT Scans',\n",
       "  'name': 'LIDC-IDRI',\n",
       "  'aliases': ['LIDC-IDRI', 'LUNA16', 'LIDC'],\n",
       "  'url': ['https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=1966254']},\n",
       " 'DRIVE': {'doi': '10.1109/TMI.2004.825627',\n",
       "  'title': 'Ridge-based vessel segmentation in color images of the retina',\n",
       "  'name': 'DRIVE',\n",
       "  'aliases': ['DRIVE'],\n",
       "  'url': ['https://drive.grand-challenge.org']},\n",
       " 'CBIS-DDSM': {'doi': '10.1038/sdata.2017.177',\n",
       "  'title': 'A curated mammography data set for use in computer-aided detection and diagnosis research',\n",
       "  'name': 'CBIS-DDSM',\n",
       "  'aliases': ['CBIS-DDSM', 'DDSM', 'CBIS'],\n",
       "  'url': ['https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=22516629']},\n",
       " 'Chexpert': {'doi': 'https://doi.org/10.1609/aaai.v33i01.3301590',\n",
       "  'title': 'CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison',\n",
       "  'name': 'Chexpert',\n",
       "  'aliases': ['Chexpert', 'CheXpert'],\n",
       "  'url': ['https://stanfordmlgroup.github.io/competitions/chexpert']},\n",
       " 'ChestX-Ray14': {'doi': 'https://doi.org/10.1109/cvpr.2017.369',\n",
       "  'title': 'ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases',\n",
       "  'name': 'ChestX-Ray14',\n",
       "  'aliases': ['ChestX-Ray14',\n",
       "   'Chest X-Ray14',\n",
       "   'Chest Xray14',\n",
       "   'ChestX-Ray8',\n",
       "   'Chest X-Ray8',\n",
       "   'Chest Xray8',\n",
       "   'NIH Chest X-ray',\n",
       "   'NIH-CXR14',\n",
       "   'NIH-CXR8',\n",
       "   'NIH CXR14',\n",
       "   'NIH CXR8',\n",
       "   'Chest-Xray14',\n",
       "   'Chest-Xray8',\n",
       "   'chestX-ray14'],\n",
       "  'url': ['https://nihcc.app.box.com/v/ChestXray-NIHCC']},\n",
       " 'PadChest': {'doi': 'https://doi.org/10.1016/j.media.2020.101797',\n",
       "  'title': 'PadChest: A large chest x-ray image dataset with multi-label annotated reports',\n",
       "  'name': 'PadChest',\n",
       "  'aliases': ['PadChest', 'PDC'],\n",
       "  'url': ['https://bimcv.cipf.es/bimcv-projects/padchest']},\n",
       " 'MIMIC': {'doi': 'https://doi.org/10.1038/sdata.2016.35,10.1097/CCM.0b013e31820a92c6,10.1109/CIC.1996.542622,10.1038/s41597-019-0322-0',\n",
       "  'title': 'MIMIC-III, a freely accessible critical care database',\n",
       "  'name': 'MIMIC',\n",
       "  'aliases': ['MIMIC', 'MIMIC-II', 'MIMIC-III', 'MIMIC-CXR'],\n",
       "  'url': ['https://archive.physionet.org/physiobank/database/mimicdb',\n",
       "   'https://archive.physionet.org/physiobank/database/mimic2cdb',\n",
       "   'https://physionet.org/content/mimiciii/1.4',\n",
       "   'https://physionet.org/content/mimic-cxr/2.0.0',\n",
       "   'https://www.physionet.org/content/mimic-cxr-jpg/2.0.0']},\n",
       " 'VinDr-CXR': {'doi': 'https://doi.org/10.48550/arxiv.2012.15029',\n",
       "  'title': \"VinDr-CXR: An open dataset of chest X-rays with radiologist's annotations\",\n",
       "  'name': 'VinDr-CXR',\n",
       "  'aliases': ['VinDr-CXR'],\n",
       "  'url': ['https://vindr.ai/datasets/cxr']},\n",
       " 'PAD-UFES-20': {'doi': 'https://doi.org/10.1016/j.dib.2020.106221',\n",
       "  'title': 'PAD-UFES-20: A skin lesion dataset composed of patient data and clinical images collected from smartphones',\n",
       "  'name': 'PAD-UFES-20',\n",
       "  'aliases': ['PAD-UFES-20'],\n",
       "  'url': ['https://data.mendeley.com/datasets/zr7vgbcyr2/1']},\n",
       " 'CAMELYON': {'doi': 'https://doi.org/10.1093/gigascience/giy065',\n",
       "  'title': '1399 H&E-stained sentinel lymph node sections of breast cancer patients: the CAMELYON dataset',\n",
       "  'name': 'CAMELYON',\n",
       "  'aliases': ['CAMELYON', 'Camelyon'],\n",
       "  'url': ['https://camelyon17.grand-challenge.org',\n",
       "   'https://registry.opendata.aws/camelyon']},\n",
       " 'CADDementia': {'doi': 'https://doi.org/10.1016/j.neuroimage.2015.01.048',\n",
       "  'title': 'Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: the CADDementia challenge',\n",
       "  'name': 'CADDementia',\n",
       "  'aliases': ['CADDementia'],\n",
       "  'url': ['https://caddementia.grand-challenge.org']},\n",
       " 'MRNet': {'doi': 'https://doi.org/10.1371/journal.pmed.1002699',\n",
       "  'title': 'Deep-learning-assisted diagnosis for knee magnetic resonance imaging: Development and retrospective validation of MRNet',\n",
       "  'name': 'MRNet',\n",
       "  'aliases': ['MRNet'],\n",
       "  'url': ['https://stanfordmlgroup.github.io/competitions/mrnet']},\n",
       " 'PROSTATEx': {'doi': 'https://doi.org/10.1117/1.jmi.5.4.044501',\n",
       "  'title': 'PROSTATEx Challenges for computerized classification of prostate lesions from multiparametric magnetic resonance images',\n",
       "  'name': 'PROSTATEx',\n",
       "  'aliases': ['PROSTATEx', 'ProstateX'],\n",
       "  'url': ['https://prostatex.grand-challenge.org',\n",
       "   'https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=23691656']}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_info = {}\n",
    "with open('../../Resources/data/datasets.csv') as ds_csv:\n",
    "    ds_reader = csv.DictReader(ds_csv)\n",
    "    for ds in ds_reader:\n",
    "        datasets_info[ds[\"name\"]] = {\n",
    "                                        \"doi\":ds[\"doi\"],\n",
    "                                        \"title\":ds[\"paper_title\"],\n",
    "                                        \"name\":ds[\"name\"],\n",
    "                                        \"aliases\":ds[\"aliases\"].split(\",\"),\n",
    "                                        \"url\":ds[\"url\"].split(\",\")\n",
    "                                     }\n",
    "datasets_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify datasets selected and sections considered as Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_columns = [\"ACDC\",\"BRATS\",\"LIDC-IDRI\",\"DRIVE\",\"PROMISE12\",\"Chexpert\",\"PadChest\",\"PAD-UFES-20\",\"CAMELYON\",\"CADDementia\",\"MRNet\",\"PROSTATEx\",\"MIMIC\",\"CBIS-DDSM\"]\n",
    "lst_keywords = [\"data\",\"method\",\"result\",\"setup\",\"material\",\"experiment\",\"evaluat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the results of grobid parsing\n",
    "xml_paths = glob.glob(\"../../Results/extraction/grobid_extraction/*\")\n",
    "\n",
    "#res will contain an element (dict) per paper, in each element keys will be datasets' name and value a list of mention location\n",
    "res = []\n",
    "for i,path in enumerate(xml_paths):\n",
    "    paper_name = path.removeprefix(\"../../Results/extraction/grobid_extraction/\").removesuffix(\".grobid.tei.xml\").removesuffix(\".txt\")\n",
    "    \n",
    "    #Check that the result is an xml, otherwise grobid had an error during the parsing and generate a .txt file\n",
    "    if path.endswith(\".xml\"):\n",
    "        #Load the xml file with BeautifulSoup for parsing\n",
    "        with open(path) as fp:\n",
    "            soup = bs.BeautifulSoup(fp,features=\"xml\")\n",
    "\n",
    "\n",
    "        #Get main section name, it will be used to assign subsection to the main one (e.g section 3.2 match with section 3.)\n",
    "        sections = {}\n",
    "        for elem in soup.findAll(\"head\"):\n",
    "            if elem.get(\"n\") and elem.get(\"n\")[0] not in sections:\n",
    "                sections[elem.get(\"n\")[0]] = elem.text.lower()\n",
    "          \n",
    "        #Search for the mention of every dataset one by one\n",
    "        for ds in datasets_info:\n",
    "            #Construct the regex for matching name or aliases, example for ACDC: (?<![^_\\W])(ACDC)|(Automated Cardiac Diagnosis Challenge)|(AC17)(?![^_\\s\\d\\.\\),'])\n",
    "            ds_urls = [f\"((https://)?{re.escape(url.removeprefix('https://').removeprefix(\"http://\"))}(/)?)\" for url in datasets_info[ds]['url']]\n",
    "            ds_name_aliases = [f\"({re.escape(a)})\" for a in datasets_info[ds][\"aliases\"]]\n",
    "            \n",
    "            ds_name_aliases_url_regex = \"|\".join(ds_name_aliases+ds_urls)\n",
    "            \n",
    "            #For every element in the xml with a matching, try to associate it with a location or a structure (Figure, Table, Footnote)\n",
    "            for elem in soup.findAll(string=re.compile(f\"(?<![^_\\\\W])({ds_name_aliases_url_regex})(?![^_\\\\s\\\\d\\\\.\\\\),'-])\")):\n",
    "                #To detect in \"normal\" text for which the parent elements is a <div>\n",
    "                parent_div = elem.find_parent(\"div\")\n",
    "                if parent_div:\n",
    "                    #If the element was part of the abstract or an appendix, the div will have a parent abstract or annex.\n",
    "                    #Otherwise it's part of a fulltext's section\n",
    "                    if parent_div.find_parent(\"abstract\"):\n",
    "                        res.append([paper_name,ds,\"In Abstract\",True])\n",
    "                    elif (parent_div.find_parent(\"div\")) and (parent_div.find_parent(\"div\").get(\"type\") == \"annex\"):\n",
    "                        res.append([paper_name,ds,\"Elsewhere\",True])\n",
    "                    elif parent_div.find(\"head\"):\n",
    "                        #Get the section number\n",
    "                        head_level = parent_div.find(\"head\").get(\"n\")\n",
    "                        \n",
    "                        #Interpolate the section in case there is no \"n\" attribute by looking at the closest previous div with this attribute\n",
    "                        if not head_level:\n",
    "                            for div in parent_div.find_previous_siblings(\"div\"):\n",
    "                                if div.find(\"head\") and div.find(\"head\").get(\"n\"):\n",
    "                                    head_level = div.find(\"head\").get(\"n\")\n",
    "                                    break\n",
    "                        \n",
    "                        #Match with a main section if it was a subsection (3.2 -> 3) and get the section name\n",
    "                        if head_level:\n",
    "                            head_text = sections[head_level[0]]\n",
    "                        else:\n",
    "                            head_text = parent_div.find(\"head\").text.lower()\n",
    "                        \n",
    "                        \n",
    "                        if any([kw in head_text for kw in lst_keywords]):\n",
    "                            to_append = \"In Method\"\n",
    "                        else:\n",
    "                            to_append = \"Elsewhere\"\n",
    "                        res.append([paper_name,ds,to_append,True])\n",
    "                \n",
    "\n",
    "                #To detect in figures or tables, for which parent element is <figure>\n",
    "                parent_figure = elem.find_parent(\"figure\")\n",
    "                if parent_figure:\n",
    "                    if parent_figure.get(\"type\") == \"table\":\n",
    "                        res.append([paper_name,ds,\"In Table\",True])                        \n",
    "                    else:\n",
    "                        res.append([paper_name,ds,\"In Figure\",True])\n",
    "                        \n",
    "\n",
    "                #To detect footnotes for which parent element is <note>\n",
    "                parent_footnote = elem.find_parent(\"note\")\n",
    "                if parent_footnote:\n",
    "                    if parent_footnote.get(\"place\") == \"foot\":\n",
    "                        res.append([paper_name,ds,\"In Footnote\",True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(res,columns=[\"doc_name\",\"label1\",\"label2\",\"value\"])\n",
    "df_res = df_res.drop_duplicates([\"doc_name\",\"label1\",\"label2\"])\n",
    "df_res.to_csv(\"../../Results/extraction/grobid_fulltext_detection.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of regex used for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r\"(?<![^_\\W])((MIMIC)|(MIMIC\\-II)|(MIMIC\\-III)|(MIMIC\\-CXR)|((https://)?archive\\.physionet\\.org/physiobank/database/mimicdb(/)?)|((https://)?archive\\.physionet\\.org/physiobank/database/mimic2cdb(/)?)|((https://)?physionet\\.org/content/mimiciii/1\\.4(/)?)|((https://)?physionet\\.org/content/mimic\\-cxr/2\\.0\\.0(/)?)|((https://)?www\\.physionet\\.org/content/mimic\\-cxr\\-jpg/2\\.0\\.0(/)?))(?![^_\\s\\d\\.\\),'])\",\n",
       "           re.UNICODE)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = \"MIMIC\"\n",
    "ds_urls = [f\"((https://)?{re.escape(url.removeprefix('https://').removeprefix(\"http://\"))}(/)?)\" for url in datasets_info[ds]['url']]\n",
    "ds_name_aliases = [f\"({re.escape(a)})\" for a in datasets_info[ds][\"aliases\"]]\n",
    "            \n",
    "\n",
    "ds_name_aliases_url_regex = \"|\".join(ds_name_aliases+ds_urls)\n",
    "regex = re.compile(f\"(?<![^_\\\\W])({ds_name_aliases_url_regex})(?![^_\\\\s\\\\d\\\\.\\\\),'])\")\n",
    "regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the results of grobid parsing\n",
    "xml_paths = glob.glob(\"../../Results/extraction/grobid_extraction/*\")\n",
    "\n",
    "#res will contain an element (dict) per paper, in each element keys will be datasets' name and value boolean indicating a citation\n",
    "res = []\n",
    "for i,path in enumerate(xml_paths):\n",
    "    paper_name = path.removeprefix(\"../../Results/extraction/grobid_extraction/\").removesuffix(\".grobid.tei.xml\").removesuffix(\".txt\")\n",
    "    \n",
    "    #Check that the result is an xml, otherwise grobid had an error during the parsing and generate a .txt file\n",
    "    if path.endswith(\".xml\"):\n",
    "        #Load the xml file with BeautifulSoup for parsing\n",
    "        with open(path) as fp:\n",
    "            soup = bs.BeautifulSoup(fp,features=\"xml\")\n",
    "          \n",
    "        #Search for the mention of every dataset one by one\n",
    "        for ds in datasets_info:\n",
    "            #Construct the regex for matching name or aliases, example for ACDC: (?<![^_\\W])(ACDC)|(Automated Cardiac Diagnosis Challenge)|(AC17)(?![^_\\s\\d\\.\\),'])\n",
    "            ds_title = re.escape(datasets_info[ds][\"title\"])\n",
    "            if soup.find(string=re.compile(ds_title,re.IGNORECASE)):\n",
    "                res.append((paper_name,ds,True))\n",
    "            else:\n",
    "                res.append((paper_name,ds,False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(res,columns=[\"doc_name\",\"label1\",\"value\"])\n",
    "df_res.to_csv(\"../../Results/extraction/grobid_reference_detection.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PublicDataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
